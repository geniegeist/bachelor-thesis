\chapter{Introduction} 

The Reconstruction Theorem allows to construct a distribution \(f\) from a family of distributions \({(F_{x})}_{x \in \mathbb{R}^d}\) such that \(f\) is locally well-approximated by \(F_x\) around \(x \in \mathbb{R}^d\). Intuitively, we may view it as a converse to Taylor's Theorem if we let \(F_x\) be a Taylor polynomial. The Reconstruction Theorem is the most fundamental theorem in the theory of regularity structures --- a novel theory proposed by Hairer~\cite{hairer2014theory} that provides a robust solution theory to many ill-posed stochastic partial differential equations. In fact, the theory of regularity structures was so groundbreaking that Hairer was awarded the Fields medal for his \emph{``creation of regularity structures''} in 2014~\cite{FieldsMedalHairer}.

The original proof of the Reconstruction Theorem relied heavily on wavelet analysis~\cite{hairer2017reconstruction}. Since then numerous, more self-contained proofs have been published; for example Otto and coauthors~\cite{otto2019quasilinear} used semigroup methods, or Friz and Hairer gave a purely local proof~\cite{friz2020course}, which generalizes to any domains. Other proofs include~\cite{gubinelli2015paracontrolled, martin2020littlewood, singh2018elementary}. Still all of these proofs require profound knowledge of regularity structures or stochastic analysis. In 2020 Caravenna and Zambotti~\cite{caravenna2021hairer} gave a truly elementary proof of the Reconstruction Theorem that uses distribution theory. 

The aim of this thesis is to give a self-contained proof of the Reconstruction Theorem. Hence, we mimic the proof by Caravenna and Zambotti. We hope that this thesis introduces the Reconstruction Theorem to a broader audience. Since the Reconstruction Theorem is a purely analytical tool, it may find applications outside regularity structures or stochastic partial differential equations. This thesis is aimed towards readers who are familiar with multivariable calculus. Knowledge of distribution theory is helpful but not mandatory since we give a concise overview of distribution theory.

The thesis is structured in the following way: Chapter~\ref{chapter:notation} introduces notation. Chapter~\ref{chapter:distributions} gives a concise overview of the theory of distributions. In Chapter~\ref{chapter:reconstruction} we state the Reconstruction Theorem with its assumptions. This leads to the central notion of \emph{coherence}, an optimal assumption coined by Caravenna and Zambotti. In Chapter~\ref{chapter:general-proof},~\ref{chapter:proof-gamma-positive} and~\ref{chapter:proof-gamma-negative} we prove the Reconstruction Theorem. We split the proof into three parts because we consider two cases: \(\gamma > 0\) and \(\gamma \leq 0\), where \({\gamma}\) is a parameter occurring in the Reconstruction Theorem. A major part of the proof holds for all \(\gamma \in \mathbb{R}\) and is presented in Chapter~\ref{chapter:general-proof}. We continue the proof for \(\gamma > 0\) in Chapter~\ref{chapter:proof-gamma-positive}, and the proof for \(\gamma \leq 0\) in Chapter~\ref{chapter:proof-gamma-negative}. In Chapter 6 we present applications of the Reconstruction Theorem, where we extend Hölder spaces to negative exponents, and prove the Sewing Lemma from rough path theory.

% ------------------------------------------

\section{Notation}\label{chapter:notation}

Let \( \mathbb{N} = \left\{ 1,2, \ldots \right\} \). Throughout this thesis, the symbol \( \mathbb{R}^d \) denotes the \( d \)-dimensional Euclidean space with the Euclidean norm \( |\cdot| \) for some \( d \in \mathbb{N} \). The \emph{open ball} \(B(x_0, r)\)  centered around \(x_0 \in \mathbb{R}^d\) with radius \(r > 0\) is defined as \(B(x_0,r) = \{ x \in \mathbb{R}^d : |x - x_0| \leq r \} \). We write \(\enlarg{A}{\epsilon}\) to enlarge a set \(A \subset \mathbb{R}^d\) by some \(\epsilon \in \mathbb{R} \): 
\begin{align*}
    \enlarg{A}{\epsilon} \coloneqq A + B(0, \epsilon) \coloneqq \{ x \in \mathbb{R}^d : |x - a| \leq \epsilon \text{ for some \(a \in A\)} \}.
\end{align*}

The \emph{multi-index notation} makes many theorems for functions in multiple variables appear as if there is only one variable. A \emph{multi-index} \(k = (k_1, \ldots ,k_d) \in \mathbb{N}^d_0\) is a \(d\)-tuple of non-negative integers. The \emph{length} of \(k\) is defined as \(|k| = k_1 + \cdots + k_d\). We define for all \(x, k,l \in \mathbb{R}^d\) and \(f: \mathbb{R}^d \to \mathbb{R}\)
\begin{gather*}
    x^k = x_1^{k_1} \cdot \cdots \cdot x_d^{k_d}, \qquad
    k! = k_1! k_2! \cdots k_d!, \qquad
    \binom{k}{l} = \binom{l_1}{k_1} \cdots \binom{l_d}{k_d} = \frac{k!}{l! (k - l)!}.
\end{gather*}
A polynomial \(f(x)\) of degree \(m \in \mathbb{N}_0\) with its partial derivatives are written as 
\begin{align*}
    f(x) &= \sum\limits_{|k| \leq m} \alpha_{k}x^k \qquad \quad \alpha_k \in \mathbb{R}  \text{ for all multi-indexes \( k \)} ,\\
    \partial^k f(x) &= \partial^{k_1}_{1} \cdots \partial^{k_d}_{d} f(x).
\end{align*}
We say 
\begin{itemize}
    \item \(f \in C\) or \(f \in C^0\) if \(f\) is continuous,
    \item \(f \in C^k\) if \(f\) is {\(k\)-times continuously differentiable} for \(k \in \mathbb{N}\), and
    \item \(f\) is {smooth} if \( f \in C^\infty \).
\end{itemize}
We define the \emph{\(C^k\)-norm} for \( f \in {C}^k \) as 
\begin{align*}
    \lVert f \rVert_{C^k} = \max\limits_{|i| \leq k} \lVert \partial^i f \rVert_{\infty},
\end{align*}
where \(\lVert f \rVert_{\infty} = \sup_{x \in \mathbb{R}^d} |f(x)|\). 

The {support} of a function \(f: \mathbb{R}^d \to \mathbb{R}\) is defined as \(\mathrm{supp}(f) = \overline{\left \{ x \in \mathbb{R}^d : f(x) \neq 0 \right \}}\), where the bar over the set denotes its closure.

Next, we state classical results from analysis in multi-index notation. We skip the proofs, and instead refer to Rudin~\cite{MR0055409}.
\begin{theorem}[Taylor's Theorem]
    Let \(f \in C^{k}(B(x_0, r))\) and \(k \in \mathbb{N}^d_0\). Then, for all \(x \in B(x_0, r)\) we have
    \begin{align*}
        f(x) = \sum\limits_{|j| \leq k}\partial^{j} f(x_0) \frac{{(x-x_0)}^j}{j!} + R(x),
    \end{align*}
    where \(\frac{R(x)}{|x-x_0|^{k}} \to 0\) as \(x \to x_0\).
\end{theorem}

\begin{theorem}[Leibniz Rule]\label{theorem:leibniz}
    The Leibniz rule is a generalization of the product rule. Let \( f, g \in {C}^k \). Then, \( fg \in {C}^k \) and for \( \alpha \in \mathbb{N}^d_0, |\alpha| \leq k \) we have
    \begin{align*}
        \partial^\alpha(fg) = \sum_{\beta \leq \alpha} \binom{\alpha}{\beta} (\partial^\beta f) (\partial^{\alpha - \beta} g).
    \end{align*}
\end{theorem}

\begin{theorem}[Mean Value Inequality]\label{mean-value-inequality}
    Let \(f: G \to \mathbb{R}\) be differentiable, where \(G\) is an open convex subset of \(\mathbb{R}^d\). Let \(a,b \in G\). Then,
    \begin{align*}
        |f(b) - f(a)| \leq \sup\limits_{x \in \overline{ab}} |f'(x)| \, |b-a|,
    \end{align*}
    where 
    \(
        f'(x) = 
        \begin{pmatrix}
            \partial_{x_1}f & \cdots & \partial_{x_d}f
        \end{pmatrix}
    \)
    is the gradient of \(f\). 
\end{theorem}

Later, we will consider functions that are said to \emph{annihilate monomials}; they play an essential role in the proof of the Reconstruction Theorem. The definition reads as follows.

\begin{definition}[Annihilation of Monomials]
    A function \(g: \mathbb{R}^d \to \mathbb{R}\) \emph{annihilates monomials} of degree \(j \in \mathbb{N}\) if for all \(n \in \mathbb{N}^d_0\) with \(|n| = j\) we have
    \begin{align*}
        \int_{\mathbb{R}^d} y^n g (y) \, \mathrm{d}y = 0.
    \end{align*}
\end{definition}

For later applications of the Reconstruction Theorem, we introduce the space of \emph{locally \({\alpha}\)-Hölder functions} \(\mathcal{C}^{\alpha}\) for positive exponents \(\alpha > 0\). It is our goal to extend this space to non-positive exponents \({\alpha \leq 0}\) in later chapters. 

\begin{definition}[Locally \({\alpha}\)-Hölder Functions]\label{definition:hoelder-functions}
    Let \(\alpha > 0\) and \(\varphi: \mathbb{R}^d \to \mathbb{R}\). We say \(\varphi \in \mathcal{C}^{\alpha}\) if
\begin{itemize}
    \item \(\varphi \in C^r\) for \(r = \max \left \{ n \in \mathbb{N}_0 : n < \alpha \right \} \), and
    \item there exists a constant \(C < {\infty}\) such that \(|\varphi(y) - F_x(y)| \leq C |y-x|^{\alpha}\) uniformly for all \(x,y\) in compact sets, where \(F_x\) is the Taylor polynomial of \({\varphi}\) of order \(r\) at \(x\).   
\end{itemize}
\end{definition}
For non-positive exponents \( \alpha \), the space \( \mathcal{C}^\alpha \) is no longer a space of continuously differentiable functions but \emph{a space of distributions}. It is now time to introduce distributions along with test functions.

% ------------------------------------------

\section{Theory of Distributions}\label{chapter:distributions}

The theory of distributions generalizes classical analysis. We no longer consider functions \( f : \mathbb{R}^d \to \mathbb{R} \) but linear forms, called \emph{distributions}, that act on \emph{test functions}. Distributions set up a general theory of partial differential equations, which classical analysis fails to do. Our goal is to quickly introduce distributions to get Reconstruction Theorem going. For a complete treatment we recommend the standard reference by Friedlander and Joshi~\cite{friedlander1998introduction}.

There are many classes of test functions. We choose the compactly supported functions \( f \in C^\infty \).

\begin{definition}[Test Function]
    \emph{Test functions} \(\varphi: \mathbb{R}^d \to \mathbb{R}\) are smooth functions that have compact support. The \emph{space of test functions} \(\mathcal{D}\) is the set that contains all test functions:
    \begin{align*}
        \mathcal{D} = \mathcal{D}(\mathbb{R}^d) &= \left \{ \varphi \in C^\infty(\mathbb{R}^d) : \text{\(\mathrm{supp}(\varphi)\) is compact} \right \}, \\
        \mathcal{D}(A) &= \left \{ \varphi \in \mathcal{D} : \mathrm{supp}(\varphi) \subset A \right \} \qquad \text{for any subset \(A \subset \mathbb{R}^d\).}
    \end{align*}
\end{definition}

The following subset of test functions play an important role in subsequent chapters
\begin{align*}
    \mathcal{B}_r = \left\{ \varphi \in \mathcal{D}(B(0,1)) : \lVert \varphi \rVert_{C^r} \leq 1 \right\}.
\end{align*}

To each class of test functions there corresponds a class of distributions.
\begin{definition}[Distribution]
A map \(u: \mathcal{D} \to \mathbb{R}\) is called \emph{distribution} if it is linear, and if for every compact set \(K \subset \mathbb{R}^d\) there exist \(r \in \mathbb{N}_0\) and \( C < \infty \) such that 
\begin{align*}
    |u(\varphi)| \leq C \lVert\varphi\rVert_{C^r}, \quad \forall \varphi \in \mathcal{D}(K).
\end{align*}
The \emph{space of all distributions} is denoted 
\(
    \mathcal{D}' = \left \{ u: \mathcal{D} \to \mathbb{R} \, | \, \text{\(u\) is a distribution} \right \}
\).
\end{definition}

Any continuous function \( f: \mathbb{R}^d \to \mathbb{R} \) can be canonically identified with a distribution by \( \varphi \mapsto \int_{\mathbb{R}^d} f(x) \varphi(x) \, \mathrm{d}x \) for all \( \varphi \in \mathcal{D} \).

We give {convergence in \(\mathcal{D}\)} a meaning.

\begin{definition}[Convergence]
    Let \((\varphi_n)\) be a sequence in \(\mathcal{D}\) and \(\varphi \in \mathcal{D}\). We say 
    \(
        \varphi_n \to \varphi  \text{ in \(\mathcal{D}\)}
    \)
    if there exists a compact set \(K \in \mathbb{R}^d\) such that \(\mathrm{supp}(\varphi) \subset K\) and \(\mathrm{supp}(\varphi_j) \subset K\) for all \(j\), and \(\lVert \varphi_n - \varphi \rVert_{C^k} \to 0\) as \(n \to {\infty}\) for all \(k \in \mathbb{N}_0\).
\end{definition}

This allows us to give an alternative characterization of distributions: a distribution is a linear functional that is {continuous}.

\begin{lemma}
    Let \(u: \mathcal{D} \to \mathbb{R}\) be a linear functional. Then, \(u\) is a {distribution} if and only if \( \varphi_j \to \varphi \) in \(\mathcal{D}\) implies \(u(\varphi_j) \to u(\varphi)\) for all test functions \(\varphi_j\) and \( \varphi \).
\end{lemma}

\begin{proof}
    Let \(u\) be a distribution and \(\varphi_j \to \varphi \) in \(\mathcal{D}\). Then, there exist \(C\) and \(r\) such that \(|u(\varphi_j -\varphi)| \leq C \lVert \varphi_j - \varphi \rVert_{C^r} \to 0\) as \(j \to \infty \). By linearity, it follows \(u(\varphi_j) \to u(\varphi)\).

    For the converse direction, we argue by contradiction. Let \( \varphi_j \to \varphi \) in \(\mathcal{D}\) imply \(u(\varphi_j) \to u(\varphi)\) for all test functions \(\varphi_j\) and \(\varphi \). Assume there is a compact set \(K \subset \mathbb{R}^d\) such that for all \(r \in \mathbb{N}_0\) and \(C < \infty \), the inequality \(|u(\varphi)| \leq C \lVert\varphi\rVert_{C^r}\) is violated for some \(\varphi \in \mathcal{D}\). Then, we can find \(\varphi_n\) with \(|u(\varphi_n)| > n \lVert \varphi_n \rVert_{C^n}\) for all \(n \in \mathbb{N}\). Next, we define \(\phi_n \coloneqq \frac{\varphi_n}{n \lVert \varphi_n \rVert_{C^n}}\). So, we get \(u(\phi_n) > 1\). However, \(\phi_n \to 0\) in \(\mathcal{D}\) as \(n \to {\infty}\) because \(\lVert \phi_n \rVert_{C^r} \leq \frac{1}{n}\) for all \(n \geq r\).
\end{proof}

Often, we are given a test function \({\varphi}\), and we would like to construct a sequence 
\((\varphi_j)\) that converges to \( \varphi \) in \( \mathcal{D} \).
Enter \emph{mollifiers}. First, we define how to scale and translate arbitrary functions \(\varphi: \mathbb{R}^d \to \mathbb{R}\):
\begin{align*}
    \varphi^\epsilon_y(x) \coloneqq \frac{1}{\epsilon^d} \, \varphi\left(\frac{x-y}{\epsilon}\right), \quad \varphi^\epsilon(x) \coloneqq \varphi^\epsilon_0(x), \quad \varphi_y(x) \coloneqq \varphi^1_y(x).
\end{align*} 

Given a compactly supported function \(\rho: \mathbb{R}^d \to \mathbb{R}\) that integrates to one, a family of scaled and translated versions of \({\rho}\) is called a \emph{mollifier}.

\begin{definition}[Mollifier]
    Let \(\rho: \mathbb{R}^d \to \mathbb{R}\) be a function with compact support and \(\int_{\mathbb{R}^d} \rho(x) \, \mathrm{d}x = 1\). We call the family of scaled functions \({(\rho^\epsilon)}_{\epsilon > 0}\) a \emph{mollifier}.
\end{definition}

We define the \emph{convolution} of two functions \(f,g \in {L}^1(\mathbb{R}^d)\) as
\begin{align*}
    (f*g)(x) = \int_{\mathbb{R}^d} f(x - y)g(y) \, \mathrm{d}y.
\end{align*}

\begin{lemma}
    Let \(f,g \in {L}^1(\mathbb{R}^d)\). Then, 
    \begin{enumerate}
        \item \(f*g\) is well-defined almost everywhere, and 
        \item \(f*g \in {L}^1(\mathbb{R}^d)\).
    \end{enumerate}
\end{lemma}
\begin{proof}
    We can safely assume \(f\) and \(g\) to be representatives of the equivalence classes, because we  treat \(\int f(x) \; \mathrm{d}x\) as Lebesgue integrals and these integrals are independent of the chosen representatives.

    First, we check that \((x,y) \mapsto f(x-y)g(y) \in L^1(\mathbb{R}^d \times \mathbb{R}^d)\) in order to apply Fubini. By Tonelli's theorem and the translation invariance of the Lebesgue integral, we obtain
    \begin{align*}
        \int_{\mathbb{R}^d \times \mathbb{R}^d}|f(x-y)g(y)| \, \mathrm{d}(x,y) = \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} |f(x-y)g(y)| \, \mathrm{d}x \, \mathrm{d}y = \lVert f \rVert_{1} \lVert g \rVert_{1} < \infty.
    \end{align*}

    Then by Fubini's theorem, we obtain that \(y \mapsto f(x-y)g(y)\) is integrable for almost every \(x \in \mathbb{R}^d\). Thus, \((f*g)(x) = \int f(x-y)g(y) \, \mathrm{d}y\) is well-defined for almost every \(x \in \mathbb{R}^d\). Also, \(f*g\) is integrable (if we assign zero in the points of the null set where it is not defined) by Fubini.
\end{proof}
Additionally, the proof shows that
\begin{align}\label{inequality:l1-norm}
    \lVert f*g \rVert_1 \leq \lVert f \rVert_{1} \lVert g \rVert_{1}.
\end{align}

When we convolute a test function \(\varphi \in \mathcal{D}\) against a mollifier \((\rho^\epsilon)\), we obtain a sequence \((\varphi * \rho^\epsilon) \subset \mathcal{D}\) that converges to \({\varphi}\) in \(\mathcal{D}\) as \(\epsilon \to 0\).  

\begin{lemma}\label{mollifier-lemma}
    Let \((\rho^\epsilon)\) be a mollifier. For all test functions \(\varphi \in \mathcal{D}\), we have
    \begin{enumerate}
        \item \(\varphi * \rho^{\epsilon} \in \mathcal{D}\) for all \(\epsilon > 0\), and
        \item \(\varphi * \rho^\epsilon \to \varphi  \text{ in \( \mathcal{D} \) as \( \epsilon \to 0 \)}\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    We first show \(\frac{\partial (\varphi * \rho^{\epsilon})}{\partial x_j} = \left(\frac{\partial \varphi}{\partial x_j}\right) * \rho^{\epsilon}\). Applying that rule inductively implies \(\varphi * \rho^{\epsilon} \in C^{\infty}\). Let \(e_j\) denote the \(j\)-th unit vector. Consider the difference quotient
    \begin{align*}
        \frac{(\varphi * \rho^\epsilon)(x + te_j) - (\varphi * \rho^\epsilon)(x)}{t} = \int \frac{\varphi(x+te_j - y) - \varphi(x-y)}{t}\rho^{\epsilon}(y) \mathrm{d}y.
    \end{align*}
    By Theorem~\ref{mean-value-inequality}, we bound
    \begin{align*}
        \frac{|\varphi (x + te_j - y) - \varphi (x-y)|}{t} \leq \max_{\xi \in [0,t]}\left |\frac{\partial}{\partial x_j}\varphi(x - y + \xi e_j) \right | \leq C
    \end{align*}
    for some \(C > 0\) since \({\varphi}\) is continuously differentiable. We now have an integrable function that dominates \(\frac{\varphi(x+te_j - y) - \varphi(x-y)}{t}\rho^{\epsilon}(y)\). We apply Lebesgue's dominated convergence theorem to obtain 
    \begin{align*}
        \frac{\partial (\varphi * \rho^{\epsilon})}{\partial x_j} = \left(\frac{\partial \varphi}{\partial x_j}\right) * \rho^{\epsilon}
    \end{align*}

    It remains to prove that \(\varphi * \rho^{\epsilon}\) has compact support. This follows because \({\varphi}\) and \(\rho^{\epsilon}\) have compact support.

    Next, we show the second claim. There exists a compact set \(K\) that contains the support of \(\varphi * \rho^{\epsilon}\) for all \(\epsilon \in (0,1)\) because \( \varphi \) and \( \rho \) have compact support. Let the support of \({\rho}\) be contained in \(B(0,r)\) for some \(r > 0\). For any multi-index \(k\), \(\epsilon \in (0,1)\) and \(x \in K\), we have
    \begin{align*}
        \partial^k(\varphi * \rho^\epsilon) (x) - \partial^k\varphi(x) = \int (\partial^k \varphi(x - y) - \partial^k \varphi(x)) \, \rho^{\epsilon}(y) \mathrm{d}y
    \end{align*}
    because \(\int \rho^{\epsilon}(y) \, \mathrm{d} y = \int \rho(y) \, \mathrm{d} y = 1\). Hence, 
    \begin{align*}
        |\partial^k(\varphi * \rho^\epsilon) (x) - \partial^k\varphi(x)| 
        &\leq \int |\partial^k \varphi(x - y) - \partial^k \varphi(x)| \, |\rho^{\epsilon}(y)| \mathrm{d}y \\
        &\Downarrow \text{Mean value theorem} \\
        & \leq \max_{z \in K_r} |\partial^{k+1}\varphi(z)|  \int  |y| \, |\rho^{\epsilon}(y)| \mathrm{d}y \\
        &\Downarrow \text{Substitution: } y = \epsilon \tilde y \\
        &= \max_{z \in K_r} |\partial^{k+1}\varphi(z)| \, \epsilon  \underbrace{\int  |\tilde y| \, |\rho(\tilde y)| \mathrm{d}\tilde y}_{< \infty}.
    \end{align*}
    Then, \(\sup_{x \in K} |\partial^k(\varphi * \rho^\epsilon) (x) - \partial^k\varphi(x)| \to 0\) as \(\epsilon \to 0\).
\end{proof}

This lemma implies that \(F(\varphi * \rho^{\epsilon}) \to F(\varphi)\) because \(F\) is a distribution.

For some locally integrable function \( g \) observe that 
\begin{align}\label{lemma:mollified-distribution}
    F(\varphi * g) = \int_{\mathbb{R}^d} F(\varphi_y)g(y) \, \mathrm{d}y
\end{align}
by linearity and Riemann sum approximation. If we let \(g = {\rho}\), we obtain the crucial relationship \(\int F(\varphi_y)\rho^{\epsilon}(y) \, \mathrm{d}y \to F(\varphi)\) as \(\epsilon \to 0\), or equivalently we have
\begin{align}\label{eq:starting-point}
    \int F(\rho_y^\epsilon) \varphi(y)\, \mathrm{d}y \to F(\varphi) \quad \text{ as } \epsilon \to 0.
\end{align}
This is \emph{key} to proving the Reconstruction Theorem. 

For future reference, we state the following corollary.
\begin{corollary}\label{cor:minosokoad}
    Let \(F \in \mathcal{D}'\) and \(g,h, \psi \in \mathcal{D}\). Then, 
    \begin{align*}
        \int_{\mathbb{R}^d} F({(g*h)}_z) \psi(z)\, \mathrm{d}z
    = \iint_{\mathbb{R}^{d \times d}} F(g_y)  h(y-z) \psi(z) \, \mathrm{d}y\, \mathrm{d}z.
    \end{align*}
\end{corollary}

\begin{proof}
    Note that \({(g*h)}_z(x) = (g*h)(x - z) = \int g(y)h(x-z-y) \, \mathrm{d}y = (g*h_z)(x)\). Using~\eqref{lemma:mollified-distribution} we get \(F({(g*h)}_z) = F(g*h_z) = \int F(g_y) h_z(y) \, \mathrm{d}y\). This proves the corollary.
\end{proof}
