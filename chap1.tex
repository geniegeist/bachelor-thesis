\chapter{Introduction} 

The Reconstruction Theorem allows to construct a distribution \(f\) from a family of distributions \({(F_{x})}_{x \in \mathbb{R}^d}\) such that \(f\) is locally well-approximated by \(F_x\) around \(x \in \mathbb{R}^d\). It may be viewed as a converse to Taylor's Theorem if \(F_x\) is a Taylor polynomial. However, this view fails to capture the true importance. The Reconstruction Theorem is the most fundamental theorem in the theory of regularity structures --- a novel theory proposed by Hairer~\cite{hairer2014theory} that provides a robust solution theory to many ill-posed stochastic partial differential equations. In fact, the theory of regularity structures was so groundbreaking that Hairer was awarded the Fields medal for his \emph{``creation of regularity structures''} in 2014~\cite{FieldsMedalHairer}. It is the Reconstruction Theorem, a purely deterministic tool from analysis, that enables the theory of regularity structures.

The original proof of the Reconstruction Theorem relied heavily on wavelet analysis; since then numerous proofs have been published~\cite{hairer2017reconstruction, otto2019quasilinear, gubinelli2015paracontrolled, martin2020littlewood, singh2018elementary}. All of these proofs required profound knowledge of regularity structures, rough path theory or paracontrolled distributions. A concise treatment of the Reconstruction Theorem for a broader audience was not available until Caravenna and Zambotti gave a proof in 2020 based on elementary distribution theory~\cite{caravenna2021hairer}. 

The aim of this thesis is to give a self-contained proof of the Reconstruction Theorem. Hence, we mimic the proof by Caravenna and Zambotti, which requires no prior knowledge. We hope that this thesis introduces the Reconstruction Theorem to a broader audience. Since the Reconstruction Theorem is a purely analytical tool, it may find applications outside regularity structures or stochastic partial differential equations.

The thesis is structured in the following way: Chapter~\ref{chapter:notation} introduces notation. Chapter~\ref{chapter:distributions} gives a concise overview of the theory of distributions. In Chapter~\ref{chapter:reconstruction} we state the Reconstruction Theorem with its assumptions. This leads to the central notion of \emph{coherence}, an optimal assumption coined by Caravenna and Zambotti. In Chapter~\ref{chapter:general-proof},~\ref{chapter:proof-gamma-positive} and~\ref{chapter:proof-gamma-negative} we prove the Reconstruction Theorem. We split the proof into three parts because we consider two cases: \(\gamma > 0\) and \(\gamma \leq 0\), where \({\gamma}\) is a parameter occuring in the Reconstruction Theorem. A major part of the proof holds for all \(\gamma \in \mathbb{R}\) and is presented in Chapter~\ref{chapter:general-proof}. We continue the proof for \(\gamma > 0\) in Chapter~\ref{chapter:proof-gamma-positive}, and the proof for \(\gamma \leq 0\) in Chapter~\ref{chapter:proof-gamma-negative}. In Chapter 6 we give an application of the Reconstruction Theorem, where we extend Hölder spaces to negative exponents. In Chapter 7, we end the thesis with a discussion of the relationship between the Reconstruction Theorem and the Sewing Lemma, another analytical tool from Rough Path theory that is often considered as the one-dimensional analogue of the Reconstruction Theorem.

% ------------------------------------------

\section{Notation}\label{chapter:notation}

Throughout this thesis, the symbol \( \mathbb{R}^d \) denotes the \( d \)-dimensional Euclidean space with the Euclidean norm \( |\cdot| \) for some \( d \in \mathbb{N} \). The \emph{open ball} \(B(x_0, r)\)  centered around \(x_0 \in \mathbb{R}^d\) with radius \(r > 0\) is defined as \(B(x_0,r) = \{ x \in \mathbb{R}^d : |x - x_0| \leq r \} \). We write \(\enlarg{A}{\epsilon}\) to enlarge a set \(A \subset \mathbb{R}^d\) by some \(\epsilon \in \mathbb{R} \): 
\begin{align*}
    \enlarg{A}{\epsilon} \coloneqq A + B(0, \epsilon) \coloneqq \{ x \in \mathbb{R}^d : |x - a| \leq \epsilon \text{ for some \(a \in A\)} \}.
\end{align*}

The \emph{multi-index notation} makes many theorems for functions in multiple variables appear as if there is only one variable. A \emph{multi-index} \(k = (k_1, \ldots ,k_d) \in \mathbb{N}^d_0\) is a \(d\)-tuple of non-negative integers. The \emph{length} of \(k\) is defined as \(|k| = k_1 + \cdots + k_d\). We define for all \(x, k,l \in \mathbb{R}^d\) and \(f: \mathbb{R}^d \to \mathbb{R}\)
\begin{gather*}
    x^k = x_1^{k_1} \cdot \cdots \cdot x_d^{k_d}, \qquad
    k! = k_1! k_2! \cdots k_d!, \qquad
    \binom{k}{l} = \binom{l_1}{k_1} \cdots \binom{l_d}{k_d} = \frac{k!}{l! (k - l)!}.
\end{gather*}
A polynomial \(f(x)\) with real coefficients \(\alpha_k \in \mathbb{R}\) of degree \(m \in \mathbb{N}_0\) in \(d\) variables can be written as 
\begin{align*}
    f(x) = \sum\limits_{|k| \leq m} \alpha_{k}x^k  \quad \text{and} \quad
    \partial^k f(x) = \partial^{k_1}_{1} \cdots \partial^{k_d}_{d} f(x).
\end{align*}
We say 
\begin{itemize}
    \item \(f \in C\) or \(f \in C^0\) if \(f\) is continuous,
    \item \(f \in C^k\) if \(f\) is {\(k\)-times continuously differentiable} for \(k \in \mathbb{N}\), and
    \item \(f\) is {smooth} if \( f \in C^\infty \).
\end{itemize}
We define the \emph{\(C^k\)-norm} as \(\lVert f \rVert_{C^k} = \max\limits_{|i| \leq k} \lVert \partial^i f \rVert_{\infty}\) where \(\lVert f \rVert_{\infty} = \sup_{x \in \mathbb{R}^d} |f(x)|\). Next, we state classical results from analysis without their proofs; the proofs can be found in any standard analysis book, for example~\cite{MR0055409}.
\begin{theorem}[Taylor's Theorem]
    Let \(f \in C^{k}(B(x_0, r))\) and \(k \in \mathbb{N}^d_0\). Then, we have \(f(x) = \sum\limits_{|j| \leq k}\partial^{j} f(x_0) \frac{{(x-x_0)}^j}{j!} + R(x)\) with \(\frac{R(x)}{|x-x_0|^{k}} \to 0\) as \(x \to x_0\) for all \(x \in B(x_0, r)\).
\end{theorem}

\begin{theorem}[Leibniz Rule]\label{theorem:leibniz}
    The Leibniz rule is a generalization of the product rule, that is
    \begin{align*}
        \partial^\alpha(fg) = \sum_{\beta \leq \alpha} \binom{\alpha}{\beta} (\partial^\beta f) (\partial^{\alpha - \beta} g).
    \end{align*}
\end{theorem}

\begin{theorem}[Mean Value Inequality]\label{mean-value-inequality}
    Let \(f: G \to \mathbb{R}\) be differentiable, where \(G\) is an open convex subset of \(\mathbb{R}^n\). Let \(a,b \in G\). Then, \(|f(b) - f(a)| \leq \sup\limits_{x \in \overline{ab}} |f'(x)| \, |b-a|\), where 
    \(
        f'(x) = 
        \begin{pmatrix}
            \partial_{x_1}f & \cdots & \partial_{x_d}f
        \end{pmatrix}
    \)
    is the gradient of \(f\). 
\end{theorem}

Later, we will consider functions that are said to \emph{annihilate monomials}; they play an essential role in the proof of the Reconstruction Theorem, and we will spend a considerate amount of time constructing such functions. The definition reads as follows.

\begin{definition}[Annihilation of Monomials]
    A function \(g: \mathbb{R}^d \to \mathbb{R}\) \emph{annihilates monomials} of degree \(j \in \mathbb{N}\) if for all \(n \in \mathbb{N}^d_0\) with \(|n| = j\) we have
    \begin{align*}
        \int_{\mathbb{R}^d} y^n g (y) \, \mathrm{d}y = 0.
    \end{align*}
\end{definition}

For later applications of the Reconstruction Theorem, we introduce the space of \emph{locally \({\alpha}\)-Hölder functions} denoted by \(\mathcal{C}^{\alpha}\) for positive exponents \(\alpha > 0\). It will be our task to extend this space to non-positive exponents \({\alpha \leq 0}\). 

\begin{definition}[Locally \({\alpha}\)-Hölder Functions]\label{definition:hoelder-functions}
    Let \(\alpha > 0\) and \(\varphi: \mathbb{R}^d \to \mathbb{R}\). We say \(\varphi \in \mathcal{C}^{\alpha}\) if
\begin{itemize}
    \item \(\varphi \in C^r\) for \(r = \max \left \{ n \in \mathbb{N}_0 : n < \alpha \right \} \), and
    \item there exists a constant \(C < {\infty}\) such that \(|\varphi(y) - F_x(y)| \leq C |y-x|^{\alpha}\) uniformly for all \(x,y\) in compact sets, where \(F_x\) is the Taylor polynomial of \({\varphi}\) of order \(r\) at \(x\).   
\end{itemize}
\end{definition}
For non-positive exponents \( \alpha \), the space \( \mathcal{C}^\alpha \) is no longer a space of continuously differentiable functions but \emph{a space of distributions}. It is now time to introduce distributions along with test functions.

% ------------------------------------------

\section{Theory of Distributions}\label{chapter:distributions}

In distribution theory one is interested in 

The beauty of~\cite{caravenna2021hairer}, on which this bachelor thesis is based, lies within the fact that the Reconstruction Theorem can be stated in terms of elementary distribution theory without the need of regularity structures. 

The first concept we will encounter is that of a \emph{support} of a function \(\varphi: \mathbb{R}^d \to \mathbb{R}\), which is defined as \(\mathrm{supp}(\varphi) = \overline{\left \{ x \in \mathbb{R}^d : \varphi(x) \neq 0 \right \}}\). 

\begin{definition}[Test Function]
    \emph{Test functions} \(\varphi: \mathbb{R}^d \to \mathbb{R}\) are smooth functions that have compact support. The \emph{space of test functions} \(\mathcal{D}\) is the set that contains all test functions:
    \begin{align*}
        \mathcal{D} = \mathcal{D}(\mathbb{R}^d) &= \left \{ \varphi \in C^\infty(\mathbb{R}^d) : \text{\(\mathrm{supp}(\varphi)\) is compact} \right \}, \\
        \mathcal{D}(A) &= \left \{ \varphi \in \mathcal{D} : \mathrm{supp}(\varphi) \subset A \right \} \qquad \text{for any subset \(A \subset \mathbb{R}^d\).}
    \end{align*}
\end{definition}

A standard example for a test function is the \emph{bump function}:
\begin{align*}
    \mathcal{B}(x) = \begin{cases}
        \exp{\left( -\frac{1}{1 - |x|^2} \right)}, \quad & |x| < 1, \\
        0, & \text{otherwise}.
    \end{cases}
\end{align*}
Clearly, the bump function has compact support in \(B(0,1)\). The proof that it is smooth can be found in any standard analysis book, e.g.\ see (22.2) in~\cite{Forster_2016}.

\emph{Distributions} are the key objects in Distribution Theory.
\begin{definition}[Distribution]
A functional \(u: \mathcal{D} \to \mathbb{R}\) is called a \emph{distribution} if \(u\) is linear, and if for every compact set \(K \subset \mathbb{R}^d\) there exist \(r \in \mathbb{N}_0\) and \( C < \infty \) such that 
\begin{align*}
    |u(\varphi)| \leq C \lVert\varphi\rVert_{C^r}, \quad \forall \varphi \in \mathcal{D}(K).
\end{align*}
The \emph{space of all distributions} is denoted 
\(
    \mathcal{D}' = \left \{ u: \mathcal{D} \to \mathbb{R} \, | \, \text{\(u\) is a distribution} \right \}
\).
\end{definition}

Next, we give \emph{convergence in \(\mathcal{D}\)} a meaning.

\begin{definition}[Convergence]
    Let \((\varphi_j)\) be a sequence in \(\mathcal{D}\) and \(\varphi \in \mathcal{D}\). We say 
    \(
        \varphi_j \to \varphi  \text{ in \(\mathcal{D}\)}
    \)
    if 
    \begin{enumerate}[label=(\roman*)] % chktex 36
        \item there exists a compact set \(K \in \mathbb{R}^d\) such that \(\mathrm{supp}(\varphi)\) and \(\mathrm{supp}(\varphi_j)\) are contained in \(K\) for all \(j\), and 
        \item \(\lVert \varphi_j - \varphi \rVert_{C^r} \to 0\) as \(j \to {\infty}\) for all \(r \in \mathbb{N}_0\).
    \end{enumerate} 
\end{definition}

This allows us to give an alternative characterization of distributions: a distribution is a linear functional that is \emph{continuous}.

\begin{lemma}
    Let \(u: \mathcal{D} \to \mathbb{R}\) be a linear functional. Then, \(u\) is a {distribution} if and only if \( \varphi_j \to \varphi \) in \(\mathcal{D}\) implies \(u(\varphi_j) \to u(\varphi)\) for all test functions \(\varphi_j\) and \( \varphi \).
\end{lemma}

\begin{proof}
    Let \(u\) be a distribution and \(\varphi_j \to \varphi \) in \(\mathcal{D}\). Then, there exist \(C\) and \(r\) such that \(|u(\varphi_j -\varphi)| \leq C \lVert \varphi_j - \varphi \rVert_{C^r} \to 0\) as \(j \to \infty \). By linearity, it follows \(u(\varphi_j) \to u(\varphi)\).

    For the converse direction, we argue by contradiction. Let \( \varphi_j \to \varphi \) in \(\mathcal{D}\) imply \(u(\varphi_j) \to u(\varphi)\) for all test functions \(\varphi_j\) and \(\varphi \). Assume, there is a compact set \(K \subset \mathbb{R}^d\) such that for all \(r \in \mathbb{N}_0\) and \(C < \infty \) the inequality \(|u(\varphi)| \leq C \lVert\varphi\rVert_{C^r}\) is violated for some \(\varphi \in \mathcal{D}\). Then, we can find \(\varphi_n\) with \(|u(\varphi_n)| > n \lVert \varphi_n \rVert_{C^n}\) for all \(n \in \mathbb{N}\). Next, we define \(\phi_n \coloneqq \frac{\varphi_n}{n \lVert \varphi_n \rVert_{C^n}}\). So, we get \(u(\phi_n) > 1\). However, \(\phi_n \to 0\) in \(\mathcal{D}\) as \(n \to {\infty}\) because \(\lVert \phi_n \rVert_{C^r} \leq \frac{1}{n}\) for all \(n \geq r\).
\end{proof}

Quite often, we are given a test function \({\varphi}\), and we would like to construct a sequence 
\((\varphi_j)\) such that 
\(
    \varphi_j \to \varphi  \text{ in \(\mathcal{D}\)}
\). 
\emph{Mollifiers} are an indispensable tool for constructing such sequences, which we will use throughout the paper. First, we need to scale and translate arbitrary functions \(\varphi: \mathbb{R}^d \to \mathbb{R}\):
\begin{align*}
    \varphi^\epsilon_y(x) \coloneqq \frac{1}{\epsilon^d} \, \varphi\left(\frac{x-y}{\epsilon}\right), \quad \varphi^\epsilon(x) \coloneqq \varphi^\epsilon_0(x), \quad \varphi_y(x) \coloneqq \varphi^1_y(x).
\end{align*} 

Given a compactly supported function \(\rho: \mathbb{R}^d \to \mathbb{R}\) that integrates to one, a family of scaled and translated versions of \({\rho}\) is called a \emph{mollifier}.

\begin{definition}[Mollifier]
    Let \(\rho: \mathbb{R}^d \to \mathbb{R}\) be a function with compact support and \(\int_{\mathbb{R}^d} \rho(x) \, \mathrm{d}x = 1\). Then, the family of scaled functions \({(\rho^\epsilon)}_{\epsilon > 0}\) is called a \emph{mollifier}.
\end{definition}

Constructing a sequence \((\varphi_j)\) such that \(\varphi_j \to {\varphi}\) in \(\mathcal{D}\) becomes an easy task with the help of mollifiers and \emph{convolutions}. The \emph{convolution} of two functions \(f,g \in {L}^1(\mathbb{R}^d)\) is defined as \((f*g)(x) = \int_{\mathbb{R}^d} f(x - y)g(y) \, \mathrm{d}y\). 

\begin{lemma}
    Let \(f,g \in {L}^1(\mathbb{R}^d)\). Then, 
    \begin{enumerate}
        \item \(f*g\) is well-defined almost everywhere, and 
        \item \(f*g \in {L}^1(\mathbb{R}^d)\).
    \end{enumerate}
\end{lemma}
\begin{proof}
    We can safely assume \(f\) and \(g\) to be representatives of the equivalence classes, because we  treat \(\int f(x) \; \mathrm{d}x\) as Lebesgue integrals and these integrals are independent of the chosen representatives.

    First, we check that \((x,y) \mapsto f(x-y)g(y) \in L^1(\mathbb{R}^d \times \mathbb{R}^d)\) in order to apply Fubini. From Tonelli's theorem and the translation invariance of the Lebesgue integral we obtain
    \begin{align*}
        \int_{\mathbb{R}^d \times \mathbb{R}^d}|f(x-y)g(y)| \, \mathrm{d}(x,y) = \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} |f(x-y)g(y)| \, \mathrm{d}x \, \mathrm{d}y = \lVert f \rVert_{1} \lVert g \rVert_{1} < \infty.
    \end{align*}

    Then, by Fubini's theorem we obtain that \(y \mapsto f(x-y)g(y)\) is integrable for almost every \(x \in \mathbb{R}^d\). Thus, \((f*g)(x) = \int f(x-y)g(y) \, \mathrm{d}y\) is well-defined for almost every \(x \in \mathbb{R}^d\). Also by Fubini, \(f*g\) is integrable (if we assign zero in the points of the null set where it is not defined).
\end{proof}
Additionally, the proof shows that
\begin{align}\label{inequality:l1-norm}
    \lVert f*g \rVert_1 \leq \lVert f \rVert_{1} \lVert g \rVert_{1}.
\end{align}

When we convolute a test function \(\varphi \in \mathcal{D}\) against a mollifier \((\rho^\epsilon)\), we obtain a sequence \((\varphi * \rho^\epsilon) \subset \mathcal{D}\) that converges to \({\varphi}\) in \(\mathcal{D}\) as \(\epsilon \to 0\).  

\begin{lemma}\label{mollifier-lemma}
    Let \((\rho^\epsilon)\) be a mollifier. For all test functions \(\varphi \in \mathcal{D}\), we have
    \begin{enumerate}
        \item \(\varphi * \rho^{\epsilon} \in \mathcal{D}\) for all \(\epsilon > 0\), and
 
        \item \(\varphi * \rho^\epsilon \to \varphi  \text{ in \( \mathcal{D} \) as \( \epsilon \to 0 \) }\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    We first show \(\frac{\partial (\varphi * \rho^{\epsilon})}{\partial x_j} = \left(\frac{\partial \varphi}{\partial x_j}\right) * \rho^{\epsilon}\). Applying that rule inductively implies \(\varphi * \rho^{\epsilon} \in C^{\infty}\). Let \(e_j\) denote the \(j\)-th unit vector. Consider the difference quotient
    \begin{align*}
        \frac{(\varphi * \rho^\epsilon)(x + te_j) - (\varphi * \rho^\epsilon)(x)}{t} = \int \frac{\varphi(x+te_j - y) - \varphi(x-y)}{t}\rho^{\epsilon}(y) \mathrm{d}y.
    \end{align*}
    By the mean value inequality (Theorem~\ref{mean-value-inequality}), we can bound
    \begin{align*}
        \frac{\varphi (x + te_j - y) - \varphi (x-y)}{t} \leq \max_{\xi \in [0,t]}\frac{\partial}{\partial x_j}\varphi(x - y + \xi e_j) \leq C
    \end{align*}
    for some \(C > 0\) since \({\varphi}\) is continuously differentiable. Thus, we found an integrable function that dominates \(\frac{\varphi(x+te_j - y) - \varphi(x-y)}{t}\rho^{\epsilon}(y)\), and we can apply Lebesgue's dominated convergence theorem to obtain \(\frac{\partial (\varphi * \rho^{\epsilon})}{\partial x_j} = \left(\frac{\partial \varphi}{\partial x_j}\right) * \rho^{\epsilon}\).

    The convolution \(\varphi * \rho^{\epsilon}\) has compact support because \({\varphi}\) and \(\rho^{\epsilon}\) have also compact support. Therefore, we conclude \(\varphi * \rho^{\epsilon} \in \mathcal{D}\).

    Now, we show \(\varphi * \rho^\epsilon \to \varphi  \text{ in \( \mathcal{D} \) as \( \epsilon \to 0 \)} \). First, there exists a compact set \(K\) that contains the support of \(\varphi * \rho^{\epsilon}\) for all \(\epsilon \in (0,1)\) because \( \varphi \) and \( \rho \) have compact support. Let the support of \({\rho}\) be contained in \(B(0,r)\) for some \(r > 0\). For any multi-index \(k\), \(\epsilon \in (0,1)\) and \(x \in K\), we have
    \begin{align*}
        \partial^k(\varphi * \rho^\epsilon) (x) - \partial^k\varphi(x) = \int (\partial^k \varphi(x - y) - \partial^k \varphi(x)) \, \rho^{\epsilon}(y) \mathrm{d}y
    \end{align*}
    because \(\int \rho^{\epsilon}(y) \, \mathrm{d} y = \int \rho(y) \, \mathrm{d} y = 1\). Hence, 
    \begin{align*}
        |\partial^k(\varphi * \rho^\epsilon) (x) - \partial^k\varphi(x)| 
        &\leq \int |\partial^k \varphi(x - y) - \partial^k \varphi(x)| \, |\rho^{\epsilon}(y)| \mathrm{d}y \\
        &\Downarrow \text{Mean value theorem} \\
        & \leq \max_{z \in K_r} |\partial^{k+1}\varphi(z)|  \int  |y| \, |\rho^{\epsilon}(y)| \mathrm{d}y \\
        &\Downarrow \text{Substitution: } y = \epsilon \tilde y \\
        &= \max_{z \in K_r} |\partial^{k+1}\varphi(z)| \, \epsilon  \underbrace{\int  |\tilde y| \, |\rho(\tilde y)| \mathrm{d}\tilde y}_{< \infty}.
    \end{align*}
    As \(\epsilon \to 0\), we see that \(\sup_{x \in K} |\partial^k(\varphi * \rho^\epsilon) (x) - \partial^k\varphi(x)| \to 0\). 
\end{proof}

Next, we can study the effect of applying a distribution \(F\) on \(\varphi * \rho^{\epsilon}\), where \({\rho}\) is a mollifier. We know that \( \varphi * \rho^{\epsilon} \to \varphi \) in \(\mathcal{D}\) as \(\epsilon \to 0\). Then, \(F(\varphi * \rho^{\epsilon}) \to F(\varphi)\) because \(F\) is a distribution. \(F(\varphi * \rho^{\epsilon})\) is also known as a \emph{mollified distribution}. We have the following result for mollified distributions.

\begin{lemma}\label{lemma:lemma-mollified-distribution}
    Let \(F \in \mathcal{D}'\). Let \(\varphi \in \mathcal{D}\) and \(g\) be locally integrable and compactly supported. Then,
    \begin{align}\label{lemma:mollified-distribution}
        F(\varphi * g) = \int_{\mathbb{R}^d} F(\varphi_y)g(y) \, \mathrm{d}y.
    \end{align}
\end{lemma}

\begin{proof}
    This follows from linearity of \(F\) and Riemann sum approximation; see Lemma 4.12 in~\cite{JanKristensenDistribution}.
    %\begin{align*}
    %    F(\varphi * g) = F\left(\int_{\mathbb{R}} \varphi(x-y) g(y) \, \mathrm{d}y\right) = F\left( 
    %        \lim_{N \to \infty} \sum^N_{k=0} \varphi(x - \xi_k)g(\xi_k)(\xi_{k+1} - \xi)
    %     \right)
    %\end{align*}
\end{proof}

If we let \(g = {\rho}\), we obtain the crucial relationship \(\int F(\varphi_y)\rho^{\epsilon}(y) \, \mathrm{d}y \to F(\varphi)\) as \(\epsilon \to 0\); or equivalently
\begin{align}\label{eq:starting-point}
    \int F(\rho_y^\epsilon) \varphi(y)\, \mathrm{d}y \to F(\varphi) \quad \text{ as } \epsilon \to 0.
\end{align}
This is \emph{key} to proving the Reconstruction Theorem. 

For future reference, we state the following corollary.
\begin{corollary}\label{cor:minosokoad}
    Let \(F \in \mathcal{D}'\) and \(g,h, \psi \in \mathcal{D}\). Then, 
    \begin{align*}
        \int_{\mathbb{R}^d} F({(g*h)}_z) \psi(z)\, \mathrm{d}z
    = \iint_{\mathbb{R}^{d \times d}} F(g_y)  h(y-z) \psi(z) \, \mathrm{d}y\, \mathrm{d}z.
    \end{align*}
\end{corollary}

\begin{proof}
    Note that \({(g*h)}_z(x) = (g*h)(x - z) = \int g(y)h(x-z-y) \, \mathrm{d}y = (g*h_z)(x)\). Using~\eqref{lemma:mollified-distribution} we get \(F({(g*h)}_z) = F(g*h_z) = \int F(g_y) h_z(y) \, \mathrm{d}y\). This proves the corollary.
\end{proof}
